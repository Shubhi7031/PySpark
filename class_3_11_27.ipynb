{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wm8JUicLksVP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "spark= SparkSession.builder.appName('day').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_df= spark.read.format('csv').option('header',True).load(\"/content/mumbai_csv.csv\")"
      ],
      "metadata": {
        "id": "X9TbVOv8kwvY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UteMrtuYnpnA",
        "outputId": "7669434f-b9b8-4acd-f14e-42a9fb051c0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+\n",
            "|month \\tsales\\tprofit\\tlocation|\n",
            "+-------------------------------+\n",
            "|           january\\t\\t5000\\t...|\n",
            "|           \\t200000\\t15000\\t...|\n",
            "|           march\\t100000\\t\\t...|\n",
            "|           april\\t\\t15000\\tm...|\n",
            "|            may\\t100000\\t5000\\t|\n",
            "|             \\t500000\\t\\tmumbai|\n",
            "|           july\\t\\t5000\\tmumbai|\n",
            "|           august\\t100000\\t5...|\n",
            "|              \\t600000\\t25000\\t|\n",
            "|           october\\t100000\\t...|\n",
            "|           november\\t\\t5000\\...|\n",
            "|           december\\t700000\\...|\n",
            "+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('day3').getOrCreate()\n",
        "csv_df=spark.read.format('csv').option('header','true').load('/content/mumbai_csv.csv')\n",
        "csv_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_ExbsZbkwqx",
        "outputId": "c45d82e7-748d-4cc7-eced-8a5959274823"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+\n",
            "|month \\tsales\\tprofit\\tlocation|\n",
            "+-------------------------------+\n",
            "|           january\\t\\t5000\\t...|\n",
            "|           \\t200000\\t15000\\t...|\n",
            "|           march\\t100000\\t\\t...|\n",
            "|           april\\t\\t15000\\tm...|\n",
            "|            may\\t100000\\t5000\\t|\n",
            "|             \\t500000\\t\\tmumbai|\n",
            "|           july\\t\\t5000\\tmumbai|\n",
            "|           august\\t100000\\t5...|\n",
            "|              \\t600000\\t25000\\t|\n",
            "|           october\\t100000\\t...|\n",
            "|           november\\t\\t5000\\...|\n",
            "|           december\\t700000\\...|\n",
            "+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('day3').getOrCreate()\n",
        "from pyspark.sql.functions import *\n",
        "data= ((1, 'charan',28),(2,'dheeraj',17),(3,'Ram',61))\n",
        "columns= ['id','name','age']\n",
        "\n",
        "df= spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "\n",
        "results= df.withColumn('status', expr(\"\"\" case\n",
        "                                          when age<=17 then 'minor'\n",
        "                                          when age>=18 and age<=60 then 'major'\n",
        "                                          else 'sr_citizen'\n",
        " end \"\"\"))\n",
        "\n",
        "results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ5gIodukwo9",
        "outputId": "0d4c6a4d-f9fb-4bd5-8981-09376d5dddcb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1| charan| 28|\n",
            "|  2|dheeraj| 17|\n",
            "|  3|    Ram| 61|\n",
            "+---+-------+---+\n",
            "\n",
            "+---+-------+---+----------+\n",
            "| id|   name|age|    status|\n",
            "+---+-------+---+----------+\n",
            "|  1| charan| 28|     major|\n",
            "|  2|dheeraj| 17|     minor|\n",
            "|  3|    Ram| 61|sr_citizen|\n",
            "+---+-------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results= df.withColumn('status', when (df.age<=17, 'minor').when((df.age>=18)&(df.age<=60),'major').otherwise('srCitizen'))\n",
        "\n",
        "results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWShMcWbkwly",
        "outputId": "4c34cb25-9442-49ee-fe8c-4a4178d2d366"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+---------+\n",
            "| id|   name|age|   status|\n",
            "+---+-------+---+---------+\n",
            "|  1| charan| 28|    major|\n",
            "|  2|dheeraj| 17|    minor|\n",
            "|  3|    Ram| 61|srCitizen|\n",
            "+---+-------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName('day1').getOrCreate()\n",
        "\n",
        "data= ((101,'aaaa', 87),(104,'bbbb',99),(109,'cccc',60))\n",
        "columns= ['id','name','marks']\n",
        "\n",
        "df= spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9fQFTC5kwkB",
        "outputId": "b4197f16-924b-4d0c-91c7-bc9a81575de0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+\n",
            "| id|name|marks|\n",
            "+---+----+-----+\n",
            "|101|aaaa|   87|\n",
            "|104|bbbb|   99|\n",
            "|109|cccc|   60|\n",
            "+---+----+-----+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- marks: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data= ((1,'charan'),(2,'bharat'),(3,'dheeraj'))\n",
        "columns= ['id','name']\n",
        "\n",
        "rdd= spark.sparkContext.parallelize(data)\n",
        "df2= rdd.toDF(columns)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsSWD4ARkwg9",
        "outputId": "e861b639-4eed-4dcd-a40c-d12353aa6384"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| charan|\n",
            "|  2| bharat|\n",
            "|  3|dheeraj|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField,StringType,IntegerType,DoubleType\n",
        "\n",
        "schema= StructType([\n",
        "    StructField('id',IntegerType(),False),\n",
        "    StructField('name', StringType(), True)\n",
        "    ])\n",
        "\n",
        "df3= spark.createDataFrame(data, schema=schema)\n",
        "df3.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYmKOXXNoNSN",
        "outputId": "3f3e18e8-456a-4c7f-a13a-4f55e619ae5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns= StructType([\n",
        "    StructField('id',IntegerType(),False),\n",
        "    StructField('name',StringType(),False)\n",
        "  ])\n",
        "\n",
        "csv_df=spark.read.format('csv').option('header','true').schema(columns).load('/content/source.csv')\n",
        "csv_df.show()\n",
        "csv_df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI0_F884kwfD",
        "outputId": "7d14ff7e-8f35-49c9-c0ba-d5c67eaf27a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|   A|\n",
            "|  2|   B|\n",
            "|  3|   C|\n",
            "|  4|   D|\n",
            "+---+----+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "par_df= spark.read.format('parquet').load('/content/data.parquet')\n",
        "\n",
        "par_df.show()\n",
        "par_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW_6D5y5lM54",
        "outputId": "4036efc9-d39a-43b3-c200-dcd3de6b60ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+----------------+\n",
            "|      id|     tdate|category|         product|\n",
            "+--------+----------+--------+----------------+\n",
            "|00000000|06-26-2011|Exercise|Gymnastics Rings|\n",
            "|00000002|06-01-2011|Exercise|Gymnastics Rings|\n",
            "+--------+----------+--------+----------------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- tdate: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orc_df=spark.read.format('orc').option('header','true').load('/content/data.orc')\n",
        "\n",
        "orc_df.show()\n",
        "orc_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45eW_qU6lMzx",
        "outputId": "c08af0bd-7c8e-4c63-c911-65772744c728"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+--------------------+------------------+-----------+----------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "|first_name|last_name|        company_name|           address|       city|    county|state|  zip|age|      phone1|      phone2|               email|                 web|\n",
            "+----------+---------+--------------------+------------------+-----------+----------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "|   Solange|   Shinko|   Mosocco, Ronald A|       426 Wolf St|   Metairie| Jefferson|   LA|70002| 21|504-979-9175|504-265-8174|  solange@shinko.com|http://www.mosocc...|\n",
            "|    Arlene|  Klusman|Beck Horizon Buil...|        3 Secor Rd|New Orleans|   Orleans|   LA|70112| 20|504-710-5840|504-946-1807|arlene_klusman@gm...|http://www.beckho...|\n",
            "|     Larae|   Gudroe|Lehigh Furn Divsn...| 6651 Municipal Rd|      Houma|Terrebonne|   LA|70360| 33|985-890-7262|985-261-5783|larae_gudroe@gmai...|http://www.lehigh...|\n",
            "| Willodean|Konopacki|            Magnuson| 55 Hawthorne Blvd|  Lafayette| Lafayette|   LA|70506| 22|337-253-8384|337-774-7564|willodean_konopac...|http://www.magnus...|\n",
            "|  Terrilyn|Rodeigues|      Stuart J Agins|    3718 S Main St|New Orleans|   Orleans|   LA|70130| 33|504-463-4384|504-635-8518|terrilyn.rodeigue...|http://www.stuart...|\n",
            "|  Kayleigh|     Lace|Dentalaw Divsn Hl...|43 Huey P Long Ave|  Lafayette| Lafayette|   LA|70508| 11|337-740-9323|337-751-2326|kayleigh.lace@yah...|http://www.dental...|\n",
            "|     Jutta|    Amyot|National Medical ...|      49 N Mays St|  Broussard| Lafayette|   LA|70518| 15|337-515-1438|337-991-8070|  jamyot@hotmail.com|http://www.nation...|\n",
            "|  Cordelia| Storment|  Burrows, Jon H Esq|    393 Hammond Dr|  Lafayette| Lafayette|   LA|70506| 16|337-566-6001|337-255-3427|cordelia_storment...|http://www.burrow...|\n",
            "+----------+---------+--------------------+------------------+-----------+----------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "\n",
            "root\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- company_name: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- zip: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- phone1: string (nullable = true)\n",
            " |-- phone2: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- web: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avro_df= spark.read.format('avro').option('header','true').load('/content/data.avro')\n",
        "\n",
        "avro_df.show()\n",
        "avro_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hGgwA7EVlMyF",
        "outputId": "c31dd4e3-1b4a-4ea6-9bce-7c027a5c8df8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3337070206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavro_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data.avro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mavro_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavro_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min,max,avg,col,expr\n",
        "\n",
        "orc_df= spark.read.format('orc').load('/content/data.orc')\n",
        "\n",
        "df1= orc_df.withColumn('age', col('age').cast(IntegerType()))\n",
        "#df1.printSchema()\n",
        "\n",
        "df= orc_df.agg(min('age').alias('min_age'),\n",
        "               max('age').alias('max_age'),\n",
        "               avg('age').alias('avg_age'))\n",
        "\n",
        "df.show()\n",
        "\n",
        "orc_df.filter(orc_df.city.contains('New')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYky5QqmlMtK",
        "outputId": "cfab77e6-bf79-451b-bf7f-e0315fe8f133"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+\n",
            "|min_age|max_age|avg_age|\n",
            "+-------+-------+-------+\n",
            "|     11|     33| 21.375|\n",
            "+-------+-------+-------+\n",
            "\n",
            "+----------+---------+--------------------+--------------+-----------+-------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "|first_name|last_name|        company_name|       address|       city| county|state|  zip|age|      phone1|      phone2|               email|                 web|\n",
            "+----------+---------+--------------------+--------------+-----------+-------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "|    Arlene|  Klusman|Beck Horizon Buil...|    3 Secor Rd|New Orleans|Orleans|   LA|70112| 20|504-710-5840|504-946-1807|arlene_klusman@gm...|http://www.beckho...|\n",
            "|  Terrilyn|Rodeigues|      Stuart J Agins|3718 S Main St|New Orleans|Orleans|   LA|70130| 33|504-463-4384|504-635-8518|terrilyn.rodeigue...|http://www.stuart...|\n",
            "+----------+---------+--------------------+--------------+-----------+-------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orc_df= orc_df.withColumnRenamed('zip','pincode')\n",
        "\n",
        "orc_df= orc_df.select('first_name','phone1','company_name','pincode')\n",
        "\n",
        "orc_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PV8BClxlMqV",
        "outputId": "87987843-688b-4815-94c9-55c24b7f66df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------------------+-------+\n",
            "|first_name|      phone1|        company_name|pincode|\n",
            "+----------+------------+--------------------+-------+\n",
            "|   Solange|504-979-9175|   Mosocco, Ronald A|  70002|\n",
            "|    Arlene|504-710-5840|Beck Horizon Buil...|  70112|\n",
            "|     Larae|985-890-7262|Lehigh Furn Divsn...|  70360|\n",
            "| Willodean|337-253-8384|            Magnuson|  70506|\n",
            "|  Terrilyn|504-463-4384|      Stuart J Agins|  70130|\n",
            "|  Kayleigh|337-740-9323|Dentalaw Divsn Hl...|  70508|\n",
            "|     Jutta|337-515-1438|National Medical ...|  70518|\n",
            "|  Cordelia|337-566-6001|  Burrows, Jon H Esq|  70506|\n",
            "+----------+------------+--------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orc_df.createOrReplaceTempView('users')\n",
        "\n",
        "results= spark.sql('select min(age) min_age, max(age) max_age from users')\n",
        "\n",
        "results.show()\n",
        "\n",
        "orc_df.count()\n",
        "orc_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "-rXHzP5llMot",
        "outputId": "654ab603-3df2-4d14-8e45-464bb318191c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `age` cannot be resolved. Did you mean one of the following? [`phone1`, `pincode`, `first_name`, `company_name`].; line 1 pos 11;\n'Project ['min('age) AS min_age#415, 'max('age) AS max_age#416]\n+- SubqueryAlias users\n   +- View (`users`, [first_name#225,phone1#234,company_name#227,pincode#370])\n      +- Project [first_name#225, phone1#234, company_name#227, pincode#370]\n         +- Project [first_name#225, phone1#234, company_name#227, pincode#370]\n            +- Project [first_name#225, last_name#226, company_name#227, address#228, city#229, county#230, state#231, zip#232 AS pincode#370, age#233, phone1#234, phone2#235, email#236, web#237]\n               +- Relation [first_name#225,last_name#226,company_name#227,address#228,city#229,county#230,state#231,zip#232,age#233,phone1#234,phone2#235,email#236,web#237] orc\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2452984245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'users'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select min(age) min_age, max(age) max_age from users'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `age` cannot be resolved. Did you mean one of the following? [`phone1`, `pincode`, `first_name`, `company_name`].; line 1 pos 11;\n'Project ['min('age) AS min_age#415, 'max('age) AS max_age#416]\n+- SubqueryAlias users\n   +- View (`users`, [first_name#225,phone1#234,company_name#227,pincode#370])\n      +- Project [first_name#225, phone1#234, company_name#227, pincode#370]\n         +- Project [first_name#225, phone1#234, company_name#227, pincode#370]\n            +- Project [first_name#225, last_name#226, company_name#227, address#228, city#229, county#230, state#231, zip#232 AS pincode#370, age#233, phone1#234, phone2#235, email#236, web#237]\n               +- Relation [first_name#225,last_name#226,company_name#227,address#228,city#229,county#230,state#231,zip#232,age#233,phone1#234,phone2#235,email#236,web#237] orc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dsl1zRhNlMlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qouSum92lMju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}